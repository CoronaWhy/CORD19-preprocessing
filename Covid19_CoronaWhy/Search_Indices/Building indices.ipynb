{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building (a) search engine(s) with Whoosh and Annoy\n",
    "\n",
    "Once we have our data produced by [CoronaWhy Team](https://www.coronawhy.org/) for [Covid19 Kaggle challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge), we can read them in and feed both into a simple search index of Whoosh and into Annoy's search forest (link). Our data consists of 40k+ papers, from which we have produced three apart data sets with original texts but split into three levels of granularity: sentences, sections and entire documents.\n",
    "\n",
    "[Whoosh](https://whoosh.readthedocs.io/en/latest/quickstart.html) is a Python pure index search engine using Okapi BM25F ranking function as well other user-defined search functions. We use it here to perform a basic word-based search.\n",
    "[Annoy](https://github.com/spotify/annoy) is a library for search in n-dimensional numerical space, e.g. word/document embeddings.\n",
    "\n",
    "The results from the indexing modules can be combined in different way: with Whoosh as a n-gram filter and Annoy as refinement, with two competing search methods etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json, os, spacy, re, gensim, string, collections, pickle, sys, time\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathos.helpers import cpu_count, freeze_support\n",
    "from pathos.multiprocessing import ProcessingPool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser, OrGroup, MultifieldParser\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_sci_lg')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "cpu_number = cpu_count()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = \"...\"\n",
    "\n",
    "list_paths_source_texts = [os.path.join(source_folder,p) for p in os.listdir(source_folder)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building search index with Whoosh\n",
    "\n",
    "With the extracted list of lemmas, and optionally UMLS terms, we can build the Whoosh index. To make use of multiprocessing, we can give in the number of available CPUs. Afterwards, we can save the whoosh object to reuse it later for search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = index_texts(paper_id_list, list_lemma, list_umls, cpu_number)\n",
    "    pickle.dump(ix,open(\"ix_whoosh_doc.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting document vectors from Scispacy\n",
    "\n",
    "To profit from advantages of semantic search, we need to gather document vectors across the whole corpus. The vectors themselves come from [a scispacy model](https://allenai.github.io/scispacy/). To make it faster, we employ once again multiprocessing with [Pathos](https://pypi.org/project/pathos/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide lists into even chunks equal to the number of processes\n",
    "content_list_chunked = chunking(content_list, cpu_number)\n",
    "paper_id_list_chunked = chunking(paper_id_list, cpu_number)\n",
    "\n",
    "results = pp.map(obtain_doc_vec, content_list_chunked, paper_id_list_chunked, list(range(len(content_list_chunked))))\n",
    "#flatten the list from multiple processes\n",
    "vectors_doc_list = [x for y in results for x in y]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Annoy forest for semantic search\n",
    "\n",
    "Finally, we can feed the doc vectors into Annoy to build a search forest of size 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AnnoyIndex(200, 'angular')\n",
    "    \n",
    "for nb, x in enumerate(vectors_doc_list):\n",
    "    t.add_item(nb, x)\n",
    "    \n",
    "t.build(10)\n",
    "t.save(\"semantic_search_doc.tree\")\n",
    "    \n",
    "print('\\n')\n",
    "print(\"---finished---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
